## CompletableFuture 的语法

1. **创建异步任务**

   - runAsync(Runnable)：异步执行，无返回值
   - supplyAsync(Supplier<T>)：异步执行，有返回值
   - 都有带 Executor 的重载：runAsync(..., executor) / supplyAsync(..., executor)

   

2. **单任务链式变换**

   - thenApply / thenApplyAsync：对结果做转换，有返回值
   - thenAccept / thenAcceptAsync：只消费结果，无返回值
   - thenRun / thenRunAsync：不关心结果，只在前一步完成后跑一段逻辑
   - thenCompose：前一个结果出来后，继续发起**下一个异步任务**（flatMap）

   

3. **多个任务的组合**

   - thenCombine / thenAcceptBoth / runAfterBoth：**两个任务都完成后**合并结果/做后续
   - applyToEither / acceptEither：**谁先完成用谁**
   - allOf：等待多个任务**全部完成**
   - anyOf：只要有一个先完成就返回

   

4. **异常处理**

   - exceptionally：出异常时返回一个**降级结果**
   - handle：成功/失败都可以统一处理并返回新结果
   - whenComplete：类似 finally，只做日志/埋点，不改结果

   

5. **结果获取/超时（JDK9+）**

   - get() / join()：阻塞拿结果
   - getNow(default)：没完成就用默认值
   - orTimeout / completeOnTimeout：超时自动 fail 或给默认值

这样一轮下来，面试官会觉得你是**按功能分类理解 API 的**，不是只会背几个方法名。





## 发货异步改造具体实施

你可以直接把你的实际场景讲出来，比如这样：

> 我们发货接口原来是串行调用多个远程服务，比如：用户标签、重货校验、标准定价、抽佣、开票、货源标签，整个链路 RT 在 1 秒多。后来用 CompletableFuture 做了并行化和降级



### 1. supplyAsync + 自定义线程池

> 首先是把串行改并行：

> 我们新建了一个 IO 线程池，比如 Executors.newFixedThreadPool(32)，然后用 supplyAsync 把几个远程调用拆开并行执行：

```java
ExecutorService ioPool = Executors.newFixedThreadPool(32);

CompletableFuture<Stock> stockFuture =
    CompletableFuture.supplyAsync(() -> stockClient.queryStock(req), ioPool);

CompletableFuture<Price> priceFuture =
    CompletableFuture.supplyAsync(() -> priceClient.queryPrice(req), ioPool);

CompletableFuture<Risk> riskFuture =
    CompletableFuture.supplyAsync(() -> riskClient.check(req), ioPool);
```

顺带还能顺出一点：“我们是 IO 密集，所以都配合自建线程池使用，没有直接用 **commonPool**。

**commonPool 的特点：**

- 线程数 ≈ CPU 核数（比如 8 核大概 8 个线程）；
- 设计初衷是跑 **短小的 CPU 计算任务**（parallelStream、ForkJoin 任务等）。

我们的场景是 IO 密集：远程 RPC / DB / HTTP 调用

- 这些任务一跑就 socket read 阻塞，线程挂那儿等网络；
- 这意味着：
  - commonPool 里的那几个线程一旦全都被“阻塞 IO”占住；
  - 后面别的用 commonPool 的东西（parallelStream、别的 CF）就**没线程可用，直接饿死**。

**所以要用“自建 IO 线程池”，好处是：**

- **隔离**：IO 任务用自己的池，不污染 commonPool；
- **可调优**：线程数可以配得比 CPU 核数大很多（比如 32、64），适配 IO 阻塞场景，而不影响 CPU 计算线程；
- **可监控**：我们可以对这个 IO 池打监控、限流、报警；
- **可按依赖拆池**：比如三方 A、三方 B 用不同线程池，某个三方挂了也不会拖死整条链路。





### 2. allOf / thenCombine 聚合多个结果

> 然后用 allOf / thenCombine 来合并结果，避免一个个 get：

```java
CompletableFuture<Void> all =
    CompletableFuture.allOf(stockFuture, priceFuture, riskFuture);

CompletableFuture<OrderContext> ctxFuture = all.thenApply(v -> {
    Stock stock = stockFuture.join();
    Price price = priceFuture.join();
    Risk risk = riskFuture.join();
    return buildContext(stock, price, risk);
});
```

你也可以说：

- 比较简单场景用 thenCombine 把两个 Future 的结果合成一个；
- 多个就用 allOf 等全部完成再汇总。





### 3. exceptionally / handle

###  做降级和兜底

> 对不那么关键的调用（比如优惠/营销），我们用了 exceptionally/handle 做降级，不让整个发货接口被它拖死：

```java
CompletableFuture<Coupon> couponFuture =
    CompletableFuture.supplyAsync(() -> couponClient.query(req), ioPool)
        .exceptionally(ex -> {
            log.warn("coupon service error", ex);
            return Coupon.empty(); // 降级为没获取到建议价
        });
```

或者：

```java
CompletableFuture<Price> priceFuture =
    CompletableFuture.supplyAsync(() -> priceClient.queryPrice(req), ioPool)
        .handle((price, ex) -> {
            if (ex != null) {
                log.error("price error", ex);
                return Price.defaultPrice();
            }
            return price;
        });
```

这里能体现出你不是只会“并行”，**还会做异常隔离和降级**。





### 4. join统一在一个地方收集结果

> 在 Controller / Service 层，最后用 join() 收集结果构造响应对象：

```
OrderContext ctx = ctxFuture.join();
return buildResponse(ctx);
```

你顺便可以补一句：

> 我们尽量不在业务代码的各个角落随便 get/join，而是把并行逻辑聚合到一块，最后统一收敛，避免到处阻塞。





### **5. 如果你用到了超时控制，也可以顺带提一下**

如果你们是 JDK 9+：

```
stockFuture.orTimeout(300, TimeUnit.MILLISECONDS)
           .exceptionally(ex -> { ... });
```

或者你可以说：

> 部分关键调用我们自己封装了带超时的 supplyAsync，超过 300ms 就走降级结果，保证发货接口整体 RT 控制在几百毫秒以内。





## 还有哪些优化手段

这个问题其实是在考你**整体性能优化思路**，而不仅仅是 CompletableFuture。你可以分两层答：

### 1. 先讲“我们自己这边还能做哪些优化”（可控部分）

可以从几个方向说：

#### ① 减少不必要的调用 / 合并调用

- **缓存**：

  - 热点数据加缓存（本地缓存 + Redis），减少每次都查三方；
  - 例如：发货时的一些静态字典、配置、价格策略直接本地缓存。

  

- **合并调用**：

  - 如果一条链路里多次调用同一个三方，考虑**批量接口**或在我们侧聚合；
  - 比如一次请求把多个货源一起问价，而不是一条一条问。



#### ② 调小串行深度：能并行的并行

- 用 CompletableFuture 把多个无强依赖的三方调用并行起来（你已经做的那部分）；
- **减少“链式同步调用”层数**，比如：
  - A 服务调 B，B 再调 C，C 再调第三方；
  - 改成 A 直接调 C/三方，或者拆出聚合服务。



#### ③ 超时 + 重试 + 降级 + 熔断

- **超时**：

  - 每个三方调用设置合理超时，比如 300ms/500ms，而不是无限等；

  

- **重试**：

  - 只有幂等的请求才允许有限次（比如1~2次）重试，并带上退避；

  

- **降级**：

  - 三方慢或失败时，用默认值 / 缓存值 / 提示用户“稍后可在记录里查看更多详情”；

  

- **熔断**：

  - 连续失败/超时过多时，短时间内直接 fail fast，不再打满三方，保护自己线程池。

这些其实配套起来才是完整的“慢依赖防护网”。



### 2. 再专门回答“三方就是慢，怎么办”

这时候可以按“业务体验层面”说几个：

#### ① 改造交互：同步改异步，接口不再等三方结果

发货链路中我们需要调用高德接口来获取该货的里程信息

> 如果三方接口本身就是 2~3 秒，且对方没法优化，那我们会考虑把这部分从同步用户接口里抽出来，用异步的方式做：

- 发货/下单接口：

  - 把用户关键操作（写我们自己的单据、校验可行性）快速完成；
  - 后台再通过 MQ / 定时任务 调用三方；

  

- 前端体验：

  - 接口立即返回“已受理/处理中”；
  - 用户在“发货记录/订单列表”里看到状态从“处理中 → 已确认”；
  - 或通过消息/推送告知结果。



**本质：** 把“用户等待 RT”从“取决于三方”变成“取决于我们自己的处理时间”。

#### ② 增加本地缓存 / 预取

适合那种“查询类”三方：

- 对一些可以预取的数据（例如路线报价、区域配置）提前拉到本地缓存；
- 实时查询时尽量先走本地缓存或我们自己的服务；
- 三方慢，只影响缓存刷新，不直接拖慢接口。



#### ③ 结果延迟一致：先给大概结果，之后再精细修正

比如：

- 发货时先按**我们本地的预估价/预估时效**返回给用户；
- 后台慢慢调三方获取更精确结果；
- 如果差异不大，用户无感知；差异大再通过消息、补差价/优惠卷之类方式处理。



#### ④ 和三方协商能力：批量接口、异步回调等

这个可以当“加分项”讲：

> 如果三方有能力改，我们会推进他们提供 **批量接口、回调接口、WebHook** 等，以减少单次 RT 和我们这边轮询的压力。





### 总结

你可以按这个版本说（根据你们实际情况微调一下就成）：

> 我们那条发货链路大量是 IO 密集的远程调用，所以在用 CompletableFuture 的时候，没有直接用默认的 ForkJoinPool.commonPool，而是配了一组专门的 IO 线程池。

> 一方面 commonPool 线程数接近 CPU 核数，适合短时间的计算任务，如果塞大量会阻塞的 HTTP/RPC 请求，很容易把 commonPool 堵死，连带影响 parallelStream、其他 CompletableFuture 链路；

> 另一方面自建线程池可以按我们发货的 QPS、RT 去调线程数、队列长度和拒绝策略，必要的时候还可以对不同三方依赖拆不同线程池，做隔离和监控。

> 对这种慢接口，除了用 CompletableFuture 并行化，我们还做了几件事情：

- > 先优化我们可控的部分，比如缓存一些三方的静态数据、合并多次调用为一次批量请求、减少链式串行调用；

- > 在三方调用上统一加了超时、有限次重试、降级逻辑和熔断保护，保证三方抖动时不会把我们的线程池拖死；

- > 对于确实无法优化、天然就慢的三方，我们会考虑把这部分改成异步处理：下单/发货接口先写入我们自己的订单并快速返回“已受理”，后续通过 MQ 调三方并更新状态，用户可以在记录里看到状态变化，必要时再用推送/短信通知。

  > 这样整体下来，既把关键路径的 RT 降下来了，也保证了在三方异常或高延迟时，我们自己的系统还能稳住。



## 线程池参数

我给你两个层次的答案：

1. 一套**通用的估算公式 + 配置思路**（你以后在面试里可以这么讲）
2. 基于你给的这个具体场景，算一版**示例配置**（有数字、有理由）



### 一、通用思路：先算「并发量」，再定线程数

先记一个简单公式（IO 密集场景非常好用）：

> **并发中的任务数 C ≈ 到达速率 λ × 平均处理时间 T**



这里分两层：

- 请求层：**发货接口的 QPS**（每秒多少次发货）
- IO 任务层：**每个请求里要并行多少个远程调用**



#### 1.1 第一步：估算峰值 QPS

你给的数据：

- 日发货量：10 万票
- 发货接口调用：我默认你是 50 万次/天（重新编辑货源）
- 高峰集中在 8–10 点、13–15 点，共 4 小时

那大概：

```
500,000 次 / 4 小时 ≈ 125,000 次/小时 ≈ 35 QPS
```

为了保险一点，**可以按 100 QPS 来预估**（留点冗余）。

> → λ_req（发货接口 QPS）≈ 100



#### 1.2 第二步：每个请求里有多少 IO 任务

你说“一次发货会有 5–10 个异步任务”，我们取 **m = 10** 比较保守。

> → λ_io（IO 调用速率）= λ_req × m ≈ 100 × 10 = 1000 次 IO 调用 / 秒



#### 1.3 第三步：估算每个 IO 调用平均耗时

这个要结合你们实际，但典型三方/内部 RPC 一般 100~300ms。

我们先按 **200ms = 0.2s** 算一版：

> **并发中的 IO 调用数：**

> C = λ_io × T ≈ 1000 × 0.2 = 200

也就是说：**在稳定状态下，平均大概有 200 个 IO 调用是“正在飞行中的”**。



#### 1.4 第四步：线程数怎么定？

IO 密集型任务，线程数可以略大于 “并发 IO 调用数”，一般经验：

> **线程数 ≈ C × 安全系数（2~4 倍）**

- C ≈ 200
- 安全系数取 2~3

所以线程数大致在 **400~600 之间**比较合理。

还要考虑 CPU：

- 比如你这台机器是 8 核：
  - 纯 CPU 任务线程数一般：8~16
  - IO 线程池可以到 3~6 倍核数：24~48
- 你算出来的 400 这个数量级是合理的，不至于被上下文切换打爆。

> ⭐ 面试口径可以说：

> 「根据发货链路峰值 QPS 和远程调用 RT，算出来并发 IO 大概在 200 左右，我们 IO 线程池配置在 320~640 这个区间，兼顾并发和上下文切换成本。」





### **二、给你一版“可以写在代码里的示例配置”**

#### 2.1 核心参数建议（结合你这个场景）

假设：

- 10 台 8 核
- 发货 QPS 峰值估算 100 QPS
- 每次发货最多 10 个 IO 子任务
- 单个远程调用 RT 200~300ms



**线程池参数可以从这样起步：**

- corePoolSize = 32
- maximumPoolSize = 64
- keepAliveTime = 60 秒
- workQueue = LinkedBlockingQueue<>(1000)
- RejectedExecutionHandler = CallerRunsPolicy 或自定义降级



解释一下：

1. **core = 32**

   - 比并发 IO 数 C/10=20 略多一点；
   - 也在 8 核 × 4 ≈ 32 这个合理范围内。

   

2. **max = 64**

   - 应对短时间突刺（比预估高一些的峰值）；
   - 不建议无限放大，避免 CPU 被上下文切换打爆。

   

3. **队列 = 1000**

   - 允许有几秒的突刺排队：
     - IO 调用速率按 100/s 算，队列 1000 ≈ 顶 10 秒的堆积；
   - 再多没有意义，堆积太多请求用户也等不下去。

   

4. **拒绝策略 = CallerRunsPolicy**

   - 当线程池 + 队列都满了，让调用方线程自己执行：

     - 形成一种**自然背压**；
     - 上游接口 RT 会变长，促使你触发限流/降级，而不是直接把任务丢掉没处理。

     

   

#### 2.2 核心代码示例（Java）

```java
int coreSize = 32;
int maxSize = 64;
long keepAliveSec = 60L;
int queueCapacity = 1000;

ThreadFactory tf = new ThreadFactoryBuilder()
        .setNameFormat("ship-io-%d")
        .setDaemon(true)
        .build();

BlockingQueue<Runnable> queue = new LinkedBlockingQueue<>(queueCapacity);

ThreadPoolExecutor shipIoExecutor = new ThreadPoolExecutor(
        coreSize,
        maxSize,
        keepAliveSec,
        TimeUnit.SECONDS,
        queue,
        tf,
        new ThreadPoolExecutor.CallerRunsPolicy()  // 拒绝时回落到调用线程执行
);
```

然后：

```
CompletableFuture.supplyAsync(() -> callStock(req), shipIoExecutor);
CompletableFuture.supplyAsync(() -> callPrice(req), shipIoExecutor);
// ...
```

> 后续根据实际监控（线程池任务队列长度、ActiveCount、任务耗时、拒绝次数）再微调 core/max/queue。





### 三、面试中可以怎么“带答案”

你可以这样回答（口语版本）：

> 我们发货链路是 IO 密集型的，一次发货会并行 5~10 个远程调用，所以我这边是先算了一下峰值场景下的并发 IO 数量，再倒推线程池配置的。

> 

> 比如我们日发货量在 10 万票左右，发货接口大概 5 万次，主要集中在早上 8–10 点和中午 1–3 点这两个高峰，那折算下来峰值 QPS 大概在 510 之间。一次发货最多 10 个异步任务，那算下来 IO 调用 QPS 在 50100 左右。

> 如果单个远程调用 RT 在 200ms 左右，IO 并发数大概在 20 个上下，我们线程池 core 配在 32，max 配在 64，基本可以覆盖峰值负载，同时不会让上下文切换把 CPU 打爆。

> 

> 队列这块我们用了一个有界队列，大概 1000 容量，再加上 CallerRunsPolicy 做背压保护，配合上游的限流和超时降级，整体在高峰时线程池不会被慢接口拖死，也能比较平滑地应对流量波动。

这样讲出来：

- 有 **公式（λ、T、C）**；
- 有 **数字**（QPS、RT、线程数）；
- 有 **工程意识**（有界队列 + 拒绝策略 + 背压）；

面试官一般就会觉得你是有“算过”的，而不是“corePoolSize=16 随便一写”的那种。