# **应急“止血”动作（10 分钟内能做的）**

## **A. 消费侧提速（优先）**

- **横向扩容消费者实例**（集群消费）：让更多实例分摊更多队列。

- **调大并发与批量**：

  - setConsumeThreadMin/Max（并发消费模式）；
  - setPullBatchSize、setConsumeMessageBatchMaxSize（一次拉/一次回调处理更多消息，注意业务幂等）。

  

- **处理“毒消息”**：

  - 降低 maxReconsumeTimes，**更快打入 DLQ**，腾出主通道；
  - 给失败异常分级：可重试 vs 不可重试（不可重试直接 DLT）。

> 如果是**顺序消费（Orderly）**：队列是**单线程**处理，**加实例也提不动该队列**。要临时放宽顺序或尽快消化“热点 key”。



## **B. 生产侧降流/削峰**

- 生产端**限速/退避**（例如失败重试间隔指数退避），避免继续把洪峰压进 Broker。
- 能做**批量发送**就做批量（同 topic 同属性合并），降低 per-message 开销。



## **C. Broker 侧“让路”**

- 同一 Topic **增加队列数**（只对**新消息**分流有效）；
- 必要时**新建临时 Topic**分流新流量；
- 资源不足就临时升配（磁盘 IOPS / CPU / 网络）。





# **对症根治（按堵点分类）**

## **1) 堵在**消费侧

- **业务处理慢**

  - 外部调用（DB/HTTP）移出临界路径：先**落盘/缓存排队**再异步加工；
  - IO 型任务用连接池/批处理（DB batch、缓存 pipeline）；
  - 引入**本地缓冲队列** + 线程池，RocketMQ 回调只做“入队”，后台并发处理。

  

- **重试过多**

  - 分类重试：校验/幂等失败这类**不可重试**直接 DLT；
  - POP 模型：**拉长 invisibleTime**，避免“刚处理就超时又投回来”的无效重试；
  - 追踪“错误 TopN”并快速修复/屏蔽。

  

- **分片不均（热点队列/热点 key）**

  - **增加队列数**并在生产端更细粒度分片（sharding key 选得更分散）；
  - 顺序要求场景：同 key 热点就真的只能慢慢吃——尽量拆 key 或按业务放宽顺序。

  

- **配置层**

  - consumeThreadMin/Max、pullBatchSize、consumeMessageBatchMaxSize、consumeTimeout 等按吞吐目标调优；
  - JVM/GC：给足堆、减少 Full GC；优化反序列化与对象分配。

  



## **2) 堵在**Broker 端

- **磁盘/刷盘瓶颈**

  - 存储用 SSD；CommitLog 与 ConsumeQueue **分盘**；
  - 刷盘改为异步（flushDiskType=ASYNC_FLUSH）提升吞吐（有可靠性取舍）；
  - 调整 OS 脏页阈值、IO 调度策略（运维向）。

  

- **调度派发落后（4.x）**

  - 检查 ReputMessage/ConsumeQueue 构建线程是否跟不上；
  - 减少过重的服务端过滤（复杂 SQL92）或改为 TAG 粗过滤。

  

- **单 Broker/Topic 热点**

  - 增加 Broker 节点、主题分区（队列）做更均衡的副本与写入分摊。

  

## **3) 堵在**生产侧

- 大消息/压缩不足：启用压缩、拆分 payload。
- 发送端重试策略过于激进：失败后短时间内**指数退避**，避免“雪上加霜”。

# 热扩容、热缩容

- **“热扩容队列”✅可以**：在线把某个 Topic 的 **queue 数量调大**，Producer/Consumer 会自动刷新路由并重平衡，新消息会落到**新增队列**，老消息还在老队列里慢慢被消费。
- **“热缩容队列”⚠️谨慎**：直接把 queue 数量调小会留下**残余未消费数据**（那些被“删掉”的队列没人再拉），容易造成**遗漏**。正确做法是**先停写这些队列→等消费干净→再缩读队列**，或者更稳妥：**新建一个 Topic** 承接流量，老 Topic 余量吃完后删除。



## 在线调整消费者队列数量

**一、RocketMQ 的队列能否在线调整？**

**1) 增加队列数（推荐、影响最小）**

- **支持在线调整**：通过管理工具更新 Topic 的队列数（读/写），Broker 会**创建新的 ConsumeQueue 文件**。
- Producer 端**定期刷新路由**（或因异常立即刷新），之后会把**新消息**按轮询/自定义选择策略发到更多队列；
- Consumer 端**自动重平衡**，把新增队列均分给各实例；**已有消息**仍在原队列里消费，**不会丢**。

**命令示例（4.x/5.x 通用的 mqadmin 思路）**：

```
# 把某 Topic 在指定 broker 上的读/写队列数扩到 16
mqadmin updateTopic -n <namesrv> -b <brokerAddr> -t MyTopic -r 16 -w 16
# 或在控制台里“编辑 Topic”直接改 read/write 队列数
```

> 注意：RocketMQ 的 Topic 是“**在每个 Master Broker 上各有 N 条队列**”，总队列数≈（broker数 × 每 broker 的队列数）。



**2) 减少队列数（不建议直接缩）**

- **不建议直接把 readQueueNums/writeQueueNums 调小**：路由刷新后，消费者**看不到被移除的队列**，那些队列里**未消费的数据会遗留**。

- **正确缩容流程**（安全版）：

  1. **先缩写队列**：把 writeQueueNums 调成目标值（例如从 16→8），**新消息只写低序号队列**；
  2. **观察消费进度**：用控制台或 mqadmin consumerProgress 看“高序号队列”的 maxOffset - consumerOffset 变为 0；
  3. **再缩读队列**：确认老队列都清空后，再把 readQueueNums 调小；必要时重启/触发一次 rebalance；
  4. 后续可做离线清理（旧队列的索引文件不再增长，但历史文件会随保留策略慢慢回收）。

  

- **更稳妥替代**：**新建一个 Topic（目标队列数）** → Producer 切换到新 Topic → 等老 Topic 消费干净后删除老 Topic。对线上扰动最小。





**二、调整队列数的副作用与注意点**

- **顺序/分片键场景**：

  如果你用“同 key 路由到同队列”来保证**键内顺序**，**变更队列数会改变取模结果**，在有“在途消息”时可能打乱顺序。

  - 解决：要么**停写→等清空→再变更**；要么自定义选择器，确保扩容后对已有 key 的队列映射在过渡期保持稳定（复杂，通常不做）。

- **消费并发与实例**：

  一个队列同时只会被**一个消费组里的一个实例**消费。**提高并发首选“加消费者实例”**；只有当“实例数≈队列数”仍不够时，再考虑“加队列”。

- **路由刷新延迟**：Producer/Consumer 路由有缓存，通常几十秒内刷新；想更快生效可以**重启/触发异常**或在控制台点“刷新路由”。

- **跨 Broker 分布**：队列数是“每个 Broker 的队列数”。扩容时注意均衡：尽量各 Master 上都同步调整，避免热点。

- **历史数据在老队列**：扩容后**老数据不会搬家**；要消化堆积，仍需消费老队列。



**三、如果只是“临时增减并发”，有更轻的办法**

很多时候你想要的是**“并发多/少一点”**，并不一定非要改“队列数”：

- **加/减消费者实例数**：最直接，触发 rebalance 后队列更均分；回收时降实例也行。
- **调消费端参数**（并发消费模式）：consumeThreadMin/Max、pullBatchSize、consumeMessageBatchMaxSize。
- **顺序消费**：无法通过线程数提升单队列吞吐（每队列单线程），这类场景的“并发扩容”只能靠**加队列**（且要评估对“键内顺序”的影响）。



**四、一个推荐“口令”**

- **要扩容吞吐**：**优先加消费者实例** → 还不够再**加队列**（热增支持）。
- **要缩容队列**：**先缩写后缩读**，等老队列清空再收口；或**新 Topic 迁移**最稳。
- **顺序业务**：变更队列数前**清空在途**，否则可能乱序。

需要的话，把你当前 **Broker 数、Topic 队列数、消费组并发/实例数、是否有顺序键**告诉我，我可以给一份**最小扰动的扩/缩容动作清单**（含 mqadmin 命令顺序与回滚点）。



## 热扩容的风险点

你现在的路由是“queueIndex = hash(id) % 队列数”。一旦**热扩容（增加队列数）**，% 队列数 的结果就会改变——同一个 id 的**后续消息会落到新的队列**，而旧消息还在老队列里没吃完，结果就是**同一 id 会同时出现在两个队列**被并行消费，**键内顺序被打破**。

短答：**会失效**。

你现在的路由是“queueIndex = hash(id) % 队列数”。一旦**热扩容（增加队列数）**，% 队列数 的结果就会改变——同一个 id 的**后续消息会落到新的队列**，而旧消息还在老队列里没吃完，结果就是**同一 id 会同时出现在两个队列**被并行消费，**键内顺序被打破**。

下面给你几种可选的应对方式，从“最省心但有停机/停写”到“无停写但实现更复杂”。



**方案 A：增加队列数**变相增加消费者实例提并发（集群消费 + 顺序模式）



**方案 B：不改队列数，只**加消费者实例提并发（集群消费 + 并发模式）（最常见）

- RocketMQ 的有序保证是“**队列内顺序**”。同一队列同一时刻只会被**一个实例**消费。
- 想提并发，**优先增实例数**（或增加 Topic 的队列数但**一开始就配足**，比如 64/128，给未来并发留余量）。
- 优点：不破坏已有路由；缺点：如果队列数已经打满与实例数相等，就到顶了。

在 **集群消费 + 并发模式（默认** **CONCURRENTLY****）** 下，**每个实例会被分配 ~5 个队列**没错，但**实例内部有一个消费线程池**（默认 20 条左右，可配 consumeThreadMin/Max）。

- 这些线程会从**被分配到的那 5 个队列**里取到的消息**并发处理**，**不是**“一个队列只配一条线程”。

```java
@Component
@RocketMQMessageListener(
    topic = "T",
    consumerGroup = "cg",
    consumeMode = ConsumeMode.CONCURRENTLY // 或 ORDERLY
)
public class MyListener implements RocketMQListener<String>, RocketMQPushConsumerLifecycleListener {
    // 自定义 Consumer，设置并发线程
    @Override
    public void prepareStart(DefaultMQPushConsumer consumer) {
        consumer.setConsumeThreadMin(10);   // 默认大约 20
        consumer.setConsumeThreadMax(30);
        consumer.setConsumeMessageBatchMaxSize(16); // 回调一次处理多条
        consumer.setPullBatchSize(64);             // 拉取批量
    }
    @Override
    public void onMessage(String msg) { /* ... */ }
}
```



**方案 C（可控切换）：**停写→清空→扩容→改路由→恢复

当你**必须**扩队列，还要保序时，做一个“微窗口”的切换：

1. **暂停生产**（或仅暂停需要保持顺序的那批 key）。
2. 等老队列**消费干净**（maxOffset - consumerOffset == 0）。
3. **updateTopic** 扩队列；更新生产端路由算法（或让它自动刷新）。
4. **恢复生产**。

- 优点：简单、顺序不乱；缺点：有短暂停写窗口。





**方案 D（更灵活）：**虚拟分片（固定桶）+ 按桶迁移

避免直接 id % 队列数，而是：

- 固定一个较大的 **VIRTUAL_BUCKETS = 1024/4096**（**永不改变**）。
- 生产端路由：bucket = hash(id) % VIRTUAL_BUCKETS，再通过**映射表**把 bucket 映到**物理队列**：

```
class BucketRouter implements MessageQueueSelector {
    final int V = 1024;
    final int[] bucketToQueue; // 由配置中心下发，可热更新
    @Override
    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        String id = (String) arg; // 你的分片键
        int bucket = Math.floorMod(id.hashCode(), V);
        int q = bucketToQueue[bucket] % mqs.size();
        return mqs.get(q);
    }
}
```

- **扩容时**：新增队列 → **挑一部分 bucket** 改映射到新队列。
- **保持顺序的关键**：**“迁移一个 bucket 前，先把该 bucket 在旧队列上的积压吃干净”**，然后再切映射（可以做成“按 bucket 切换”的小批次热迁，无需全量停写）。
- 优点：无全局停写、可渐进扩容；缺点：要有**映射表**和“**按 bucket 对账/清空**”的运维流程。

> 一定记住：**不要直接从 id % N 跳到 id % (N+M)**，那是所有 key 全部漂移，顺序必乱。



**方案 E（“新老并行”）：**新 Topic 承接新键，老键留在老 Topic

- 新建一个 Topic（更多队列），**仅把“新产生的 id/会话”路由到新 Topic**；老 id 继续走老 Topic，等其生命周期结束再收口。
- 优点：不影响存量顺序；缺点：需要双 Topic 运营、路由策略要能区分“新老键”。