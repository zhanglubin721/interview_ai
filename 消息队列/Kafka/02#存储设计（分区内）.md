# **1) 分区在磁盘上的文件集合（每个 segment 一组）**

Kafka 的 topic-partition 在磁盘是一个目录；目录里按 **segment** 切分为多组文件（同前缀是一个 segment，前缀=该段的 base offset，例如 00000000000000123456.*）：

- *.log：数据本体（Record Batches）。

- *.index：**offset 索引**，把“逻辑 offset → 该段 .log 文件内的字节位置”。条目是稀疏的。

- *.timeindex：**时间索引**，把“时间戳 → offset”（再借 offset 索引定位字节位置）。

- *.snapshot：生产者状态快照（producerId/sequence），用于幂等/事务恢复。

- 目录里还会见到 leader-epoch-checkpoint（leader epoch→offset 映射，用于副本一致性/截断）。

  这些都能在官方与社区文章的目录截图与说明中看到。 

> 补充：Dump 工具能把段内的 batch 头部（baseOffset、CreateTime、isTransactional 等）打印出来，验证结构。 





# **2)** **.index**（offset 索引）——稀疏、定长、相对 offset

**结构与容量**

- **每条 8 字节**：4B 相对 offset（相对本段 baseOffset） + 4B 物理位置（字节）。 
- **稀疏写入**：默认 **每向 .log 追加 ~4096B 数据，就追加一条索引**（log.index.interval.bytes=4096）。 
- 索引文件 **预分配**，默认最大 **10 MiB**（log.index.size.max.bytes），段若把索引写满也会**强制滚动**。 



**为什么是“相对 offset（int）”**

用 4B 存相对 offset（而不是 8B 绝对 offset）可把条目压到 8B；查找时再加回 baseOffset。实现与说明可见源码/内部解读。 



**查找流程（按 offset 定位）**

1. 先按文件名前缀选择 segment（baseOffset ≤ 目标 offset < 下一段 baseOffset）。

2. 在该段的 .index 用二分搜到 **不大于目标 offset 的最近条目**，得到一个“锚点字节位置”。

3. 从 .log 的该位置顺序扫到精确的 batch/record。

   （Strimzi 的演示图和 Dump 索引输出能看到“比数据量少很多的稀疏条目”。） 







# **3)** **.timeindex**（时间索引）——按时间戳反查 offset

**结构与用途**

- **每条 12 字节**：8B timestamp + 4B offset。 
- 支持 “按时间戳起始消费”（客户端 offsetsForTimes 等 API 会走这里）。 

**与滚动的关系**

因为单条是 12B，**更容易先写满**，从而比 .index 更早触发段滚动（受 log.index.size.max.bytes 共同限制）。 





# **4) 索引写入与**段滚动的三个触发器

一个 segment 何时关闭并开启下一个？

1. **按大小**：log.segment.bytes（默认 1 GiB）。

2. **按时间**：log.roll.ms / log.roll.hours（默认 7 天，可叠加 log.roll.jitter.ms 防止羊群效应）。

3. **索引写满**：.index 或 .timeindex 达到 log.index.size.max.bytes 也会滚动。

   （这三点与默认值、推导例子在 Strimzi 的文章里有详细推演。） 

> 参数联动示例：默认 index.interval.bytes=4096，1 GiB 段大约产生 262,144 个 offset 索引条目 ≈ 2 MiB（足够小，不会先写满 10 MiB 上限）；但若你把段调到 **>5 GiB** 又不调大索引上限，就会被**索引先写满**而被迫滚动。 





# **5) 事务/幂等相关的两类辅助文件**

- **.txnindex**（可选）：**事务索引**，仅当该段存在**事务性写入**时创建，**记录本段内已中止事务的 offset 范围**，供 isolation.level=read_committed 的消费者跳过中止数据。 
- **.snapshot**：**生产者状态快照**（producerId/sequence），用于 leader 变更或恢复时重建幂等/事务所需的 producer state。 
- 另外目录中的 **leader-epoch-checkpoint** 记录 **(epoch, offset)** 对，用于副本跟随/截断与一致性校验（tiered storage 与副本恢复也会依赖它）。 







# **6) 实现细节与性能要点**

- **索引使用 mmap**（MappedByteBuffer），使二分查找与顺序追加开销极低；数据文件（.log）走 FileChannel/page cache/零拷贝传输；这是 “小索引 mmap + 大数据顺写” 的常见折中。 
- **索引条目是稀疏的**：查找通常是“索引二分 + 小段顺扫”，扫的距离由 log.index.interval.bytes 与记录平均大小决定（间隔越小，查找越快、索引越大）。 





# **7) 关键参数怎么调（工程建议）**

- **log.index.interval.bytes**（默认 4096）：

  - 小消息（≤1 KB）+ 低延迟查找 → 可降到 2 KB 或 1 KB；
  - 大消息（≥10 KB）→ 保持默认以免索引膨胀。 

  

- **log.index.size.max.bytes**（默认 10 MiB）：

  - 若把 log.segment.bytes 拉到 **>5 GiB**，**一定**要同步增大它（否则会被“索引写满”提前滚动）。 

- **滚动相关**：log.segment.bytes / log.roll.ms(hours) / log.roll.jitter.ms 按你的冷热数据分布与保留策略来配；大量分区+同一时间滚动时，用 jitter 打散。 

- **时间查找体验**取决于 .timeindex 的粒度（仍由 index.interval.bytes 驱动）和消息的时间戳模式（CreateTime vs LogAppendTime）。





# **8) 一些边界与坑**

- **相对 offset 的 int 上限**：单段内 offset - baseOffset 不能超过 Integer.MAX_VALUE；历史上在极端 compaction 场景有过超界 bug 讨论，现代版本会通过滚动/校验避免。 
- **.timeindex** **先满**导致比预期更早滚动：因为单条 12B，比 .index 大 1.5 倍，注意与 segment.bytes 的配比。 
- **事务读取**：read_committed 才会利用 .txnindex 跳过中止事务；read_uncommitted 则忽略。 





# **9) 读路径 & 写路径（速写）**

- **写入**：追加 batch→按 index.interval.bytes 决定是否落一条 .index 条目→更新 .timeindex 的时间戳→必要时滚段（大小/时间/索引满）。 
- **读取（按 offset）**：选段→.index 二分→跳到 .log 字节位置顺扫→若 read_committed 则参考 .txnindex 跳过中止数据→返回。 
- **读取（按时间）**：.timeindex 找到 “≥T 的首个 offset” → 走 offset 路径。 







## **小结**

- .index：8B/条（相对 offset + 位置），稀疏、mmap、二分查找；

- .timeindex：12B/条（时间戳 + offset），支持按时间找位点；

- 段滚动除大小/时间外，还受 **索引写满** 触发；

- 事务/幂等会看到 .txnindex 与 .snapshot；目录里还有 leader-epoch-checkpoint 维护副本一致性。

  以上关键点及默认值/尺寸均可从官方/权威文章的示例与参数解释中对应到具体证据。 

> 如果你愿意，把你线上主题的 segment.bytes、消息平均大小、是否需要按时间查找（比如延迟追溯）告诉我，我可以按你的吞吐/查找延迟目标给一份 **index.interval / index.size.max / 滚动策略**的具体配比建议。