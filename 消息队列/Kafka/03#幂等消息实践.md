# 生产者

## 发送事务消息

```java
// Producer 端（幂等 + 事务）
Properties p = new Properties();
p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "..."); 
p.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");       // 幂等
p.put(ProducerConfig.ACKS_CONFIG, "all");
p.put(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE));
p.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5"); // Kafka 1.1+ 下5即可保证幂等（更早版本需=1）
p.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "order-svc-tx-01"); // 每个实例唯一

KafkaProducer<String, String> producer = new KafkaProducer<>(p);
producer.initTransactions();

// 一个 read-process-write 事务示例
producer.beginTransaction();
try {
    // 1) 处理后写出多个分区/主题
    producer.send(new ProducerRecord<>("out-topic", key, value));
    // 2) 将本批消费进度也纳入同一事务（与下游写原子提交）
    producer.sendOffsetsToTransaction(offsets, new ConsumerGroupMetadata("cg-xxx"));
    producer.commitTransaction();
} catch (Exception e) {
    producer.abortTransaction();
}
```

幂等生产者

- 生产者启动后向集群申请一个 **ProducerId (PID)**。
- 对每个 **(分区)**，生产者在每个批次上带一个**递增的序号**（sequence number）。
- Broker 为每个 **(PID, 分区)** 维护“**最后一次成功的序号**”。
  - **重复提交**（序号相同）→ 直接丢弃（去重）。
  - **越序提交**（跳号/乱序）→ 拒绝并返回 OutOfOrderSequence 类错误，生产者按协议处理（必要时重连/刷新 PID）。

# 消费者

## 关闭自动提交

```yaml
spring:
  kafka:
    consumer:
      bootstrap-servers: ${KAFKA_BOOTSTRAP}
      group-id: cg-order
      enable-auto-commit: false         # 关闭自动提交
      isolation-level: read_committed   # 上游用事务时建议
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      max-poll-records: 100
      max-poll-interval-ms: 300000
```



## 消费者自定义监听器

```java
@Bean
ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory(
        ConsumerFactory<String, String> cf,
        KafkaTemplate<Object, Object> template
) {
    var f = new ConcurrentKafkaListenerContainerFactory<String, String>();
    f.setConsumerFactory(cf);

    // 关键：手动 ack（MANUAL / MANUAL_IMMEDIATE 二选一，见下）
    f.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);

    // 若需要“乱序 ack”支持（一次 poll 返回多条记录，允许按分区乱序确认）
    f.getContainerProperties().setAsyncAcks(true); // 2.8+ 支持，容器会延迟提交直至缺失的 ack 都到位
    f.getContainerProperties().setPollTimeout(1500);

    // 错误处理：阻塞重试 + DLT
    f.setCommonErrorHandler(defaultErrorHandler(template));

    // 并发度按分区数设置（保证每个分区单线程）
    f.setConcurrency(Runtime.getRuntime().availableProcessors());
    return f;
}

@Bean
CommonErrorHandler defaultErrorHandler(KafkaTemplate<Object, Object> template) {
    // 默认 DLT 主题名：<原topic>.DLT，分区：与源分区相同
    var recoverer = new DeadLetterPublishingRecoverer(
        template,
        (rec, ex) -> new TopicPartition(rec.topic() + ".DLT", rec.partition())
    );
    var eh = new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 5L)); // 1s 间隔重试5次
    // 指定“不重试”异常（直接进 DLT）
    eh.addNotRetryableExceptions(IllegalArgumentException.class);
    return eh;
}
```



## 消费者消费

```java
@Service
public class OrderEventListener {

    private final OrderService orderService;

    @KafkaListener(
        topics = "order-events",
        groupId = "cg-order",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void onMessage(ConsumerRecord<String, String> r, Acknowledgment ack) {
        // 业务异常抛出 → 不 ack → 交给错误处理器重试/进 DLT
        orderService.process(r); // 内部 @Transactional
        ack.acknowledge();       // 仅当 DB 事务成功才提交 offset
    }
}
```





## 重试规则



### **A) 阻塞式重试（**DefaultErrorHandler + BackOff**的那种）**

**核心思路**：同一条消息在**原主题/原分区**里被**同一个消费线程**反复处理，直到成功或次数用尽。

### **具体怎么做？**

1. 监听方法抛异常 → 进入 DefaultErrorHandler
2. 错误处理器：
   - 记录这条记录已第 *n* 次失败；
   - **对失败的分区执行** **seek** **到该条 offset**（保证下一次 poll() 还是读到它）；
   - **按 BackOff 等待一段时间**（例如 1s、2s…）；
3. 线程再次 poll() → 因为 offset 没前进，**还是这条消息** → 重试处理；
4. 超过最大次数仍失败 → **把这条消息作为“新消息”生产到 DLT**（死信主题），**然后提交原分区的 offset**，主流程继续。

> 重要细节

- **Broker 并没有“重试计数/重投队列”**：它只是看到**你没提交 offset**，所以下一轮还是把**同一条**交给你。

- **等待在哪里发生？** 就在**消费者线程**里（由 Spring 的错误处理器控制）。

- **会不会阻塞别的消息？**

  - **同一个分区**肯定阻塞（顺序被保护）。
  - 如果**同一个线程**还绑定了其他分区，阻塞式等待会让这个线程在等待期间**也无法处理那些分区**——所以生产上通常把 concurrency 调到“**每个线程尽量只管少量分区**”，或对强顺序业务用“**每分区=1 线程**”的近似配置。

- **max.poll.interval.ms** **风险**：总的等待时间（重试次数×间隔 + 处理时长）不能超过它，否则会被判定为“长时间不 poll”而触发 rebalance。

  - 所以：**阻塞式重试适合重试次数少、间隔短、要强顺序的场景**；否则用非阻塞式。

  



### **B) 非阻塞式重试（**@RetryableTopi多级重试主题 + DLT）**

**核心思路**：**不要卡住原分区**。失败的记录**被转发到“重试主题”**，原主题的 offset **立即提交**，主消费继续向前。

### **具体怎么做？**

1. 监听方法抛异常 → 框架**把这条消息（连同异常信息等 headers）生产到** **topic-retry-0**（第一次重试队列），**并提交原主题的 offset**；
2. 针对 topic-retry-0 有**独立的消费者容器**；框架根据你的 backoff 策略**控制这条消息何时才会被该容器实际投递处理**（达到延迟再消费）；
3. 在 topic-retry-0 再次失败 → **转发到** **topic-retry-1**（更长延迟）…直到次数用尽 → **发到 DLT**。

> 重要细节

- **Kafka 仍然没有“服务端延迟队列”**——“延迟”效果是由**消费者容器**来实现的（比如对重试主题的分区做**定时/暂停**直到到期再处理）。
- **每一级重试/死信都是**“**新生产的一条消息**”，所以它们有**自己的 offset**，与原主题互不影响。
- 因为原主题 offset 已经提交，**主消费不会被阻塞**；代价是**同 key 的顺序可能被打乱**（失败的那条绕去重试轨道，后面的正常消息在主轨道继续）。



### @RetryableTopic 多级重试

```java
@Service
public class OrderListener {

    // 多级重试 + 退避；到顶后进 DLT（<topic>.DLT）
    @RetryableTopic(
        attempts = "4",                                  // 主 + 3级重试
        backoff = @Backoff(delay = 1000, multiplier = 2) // 1s, 2s, 4s...
        // include/exclude 可精确控制哪些异常走重试/直接进DLT
    )
    @KafkaListener(
        topics = "order-events",
        groupId = "cg-order",
        containerFactory = "kafkaListenerContainerFactory" // 你的手动ack工厂
    )
    public void onMessage(
        ConsumerRecord<String, String> r,
        Acknowledgment ack,
        // 这些 header 方便你区分来自主/重试/DLT
        @Header(name = KafkaHeaders.RECEIVED_TOPIC, required = false) String topic,
        @Header(name = KafkaHeaders.DELIVERY_ATTEMPT, required = false) Integer attempt
    ) {
        // 你的业务逻辑，失败就抛异常；成功后再 ack
        handleAndWriteDb(r);
        ack.acknowledge();
    }

    // 可选：专门处理 DLT（只在 @RetryableTopic 管理的 DLT 上触发）
    @DltHandler
    public void onDlt(ConsumerRecord<String, String> r,
                      @Header(KafkaHeaders.DLT_EXCEPTION_MESSAGE) String exMsg) {
        // 告警/人工干预/补偿...
    }
}
```

